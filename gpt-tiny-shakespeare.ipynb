{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# load data into pandas dfs\nroot_dir = '/kaggle/input/the-bards-best-a-character-modeling-dataset/'\n\ntrain_file = 'train.csv'\ntest_file = 'test.csv'\nval_file = 'validation.csv'\n\ndf_train = pd.read_csv(root_dir+train_file)\ndf_test = pd.read_csv(root_dir+test_file)\ndf_val = pd.read_csv(root_dir+val_file)\n\n\nprint(df_train)\nprint(df_test)\nprint(df_val)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-26T14:26:31.065492Z","iopub.execute_input":"2023-08-26T14:26:31.066041Z","iopub.status.idle":"2023-08-26T14:26:34.656540Z","shell.execute_reply.started":"2023-08-26T14:26:31.065989Z","shell.execute_reply":"2023-08-26T14:26:34.655264Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"                                                text\n0  First Citizen:\\nBefore we proceed any further,...\n                                                text\n0  rance ta'en\\nAs shall with either part's agree...\n                                                text\n0  ?\\n\\nGREMIO:\\nGood morrow, neighbour Baptista....\n","output_type":"stream"}]},{"cell_type":"code","source":"# parameters\nbatch_size = 32  # 64\nblock_size = 8  # 256\nmax_iter = 5000\neval_interval = 500\nlearning_rate = 1e-3  # 3e-4\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\neval_iters = 200\nn_embed = 32  # 384\n# n_head = 6\n# n_layer = 6\n# dropout = 0.2\nhead_size = 16","metadata":{"execution":{"iopub.status.busy":"2023-08-26T14:26:37.915042Z","iopub.execute_input":"2023-08-26T14:26:37.916180Z","iopub.status.idle":"2023-08-26T14:26:37.982923Z","shell.execute_reply.started":"2023-08-26T14:26:37.916141Z","shell.execute_reply":"2023-08-26T14:26:37.981797Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# explore datset and retrive characters in text\ntrain_text = df_train['text'].iloc[0]\ntest_text = df_test['text'].iloc[0]\nval_text = df_val['text'].iloc[0]\n\nall_text = train_text+test_text+val_text\nchars = sorted(list(set(all_text)))\nvocab_size = len(chars)\n\n# map chars to ints + reverse\nstoi = {ch:i for i, ch in enumerate(chars)}\nitos = {i:ch for i, ch in enumerate(chars)}\n\n# enc/dec pair for int->str + str->int\nencode = lambda s: [stoi[c] for c in s]\ndecode = lambda l: [itos[i] for i in l]\n\n# create encoded datasets\ntrain_data = torch.tensor(encode(train_text), dtype=torch.long)\ntest_data = torch.tensor(encode(test_text), dtype=torch.long)\nval_data = torch.tensor(encode(val_text), dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T14:26:40.867816Z","iopub.execute_input":"2023-08-26T14:26:40.868172Z","iopub.status.idle":"2023-08-26T14:26:41.158730Z","shell.execute_reply.started":"2023-08-26T14:26:40.868142Z","shell.execute_reply":"2023-08-26T14:26:41.157775Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# get batch of tokens\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n    idx = torch.randint(len(data)-block_size, (batch_size, ))\n    \n    x = torch.stack([data[i:i+block_size] for i in idx])\n    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n    return x.to(device), y.to(device)\n\n@torch.no_grad()\ndef estim_loss():\n    out = {}\n    model.eval()\n    \n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        \n        for k in range(eval_iters):\n            X, y = get_batch(split)\n            logits, loss = model(X, y)\n            losses[k] = loss.item()\n            \n        out[split] = losses.mean()\n    model.train()\n    return out\n\nxb, yb = get_batch('train')\n","metadata":{"execution":{"iopub.status.busy":"2023-08-26T14:26:43.514769Z","iopub.execute_input":"2023-08-26T14:26:43.515785Z","iopub.status.idle":"2023-08-26T14:26:46.812817Z","shell.execute_reply.started":"2023-08-26T14:26:43.515722Z","shell.execute_reply":"2023-08-26T14:26:46.811767Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class SingleHeadAttention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embed, head_size, bias=False)\n        self.query = nn.Linear(n_embed, head_size, bias=False)\n        self.value = nn.Linear(n_embed, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        \n    def forward(self, x):\n        B, T, C = x.shape\n\n        k = self.key(x)\n        q = self.query(x)\n        v = self.value(x)\n\n        w = q@k.transpose(-2, -1)\n        w = w.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n        w = F.softmax(w, dim=-1)*head_size**-0.5\n\n        out = w@v\n        return out\n\n    \nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([SingleHeadAttention(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embed, n_embed)\n    \n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.proj(out)\n        return out\n\n    \nclass FeedForward(nn.Module):\n    def __init__(self, n_embed):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embed, 4*n_embed),\n            nn.ReLU(),\n            nn.Linear(4*n_embed, n_embed)\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n\n    \nclass TransformerBlock(nn.Module):\n    def __init__(self, n_embed, num_heads):\n        super().__init__()\n        head_size = n_embed//num_heads\n        self.sa = MultiHeadAttention(num_heads, head_size)\n        self.ffwd = FeedForward(n_embed)\n        self.ln1 = nn.LayerNorm(n_embed)\n        self.ln2 = nn.LayerNorm(n_embed)\n        \n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-08-26T14:26:49.776364Z","iopub.execute_input":"2023-08-26T14:26:49.776741Z","iopub.status.idle":"2023-08-26T14:26:49.792769Z","shell.execute_reply.started":"2023-08-26T14:26:49.776691Z","shell.execute_reply":"2023-08-26T14:26:49.791768Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n        \n        self.transformer_blocks = nn.Sequential(\n            TransformerBlock(n_embed, num_heads=4),\n            TransformerBlock(n_embed, num_heads=4),\n            TransformerBlock(n_embed, num_heads=4),\n            nn.LayerNorm(n_embed),\n        )\n        self.ffwd = FeedForward(n_embed)\n        self.lm_head = nn.Linear(n_embed, vocab_size)\n    \n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tokens_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n        \n        x = tokens_emb + pos_emb\n        x = self.transformer_blocks(x)\n        logits = self.lm_head(x)\n    \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n            \n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            print(idx_cond)\n            logits, loss = self(idx_cond)\n            logits = logits[: -1, :]\n            probs = F.softmax(logits, dim=-1)\n            print(probs)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat([idx, idx_next], dim=1)\n        return idx","metadata":{"execution":{"iopub.status.busy":"2023-08-26T14:26:53.854340Z","iopub.execute_input":"2023-08-26T14:26:53.854737Z","iopub.status.idle":"2023-08-26T14:26:53.866045Z","shell.execute_reply.started":"2023-08-26T14:26:53.854688Z","shell.execute_reply":"2023-08-26T14:26:53.865045Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model = BigramLanguageModel().to(device)\n\noptimiser = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor i in range(max_iter):\n    if (i+1) % eval_interval == 0:\n        losses = estim_loss()\n        print(f'step {i+1:4d}: train loss {losses[\"train\"]:.4f}, val loss {losses[\"val\"]:.4f}')\n        \n    xb, yb = get_batch('train')\n    \n    logits, loss = model(xb, yb)\n    optimiser.zero_grad(set_to_none=True)\n    loss.backward()\n    optimiser.step()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T14:26:57.861063Z","iopub.execute_input":"2023-08-26T14:26:57.861504Z","iopub.status.idle":"2023-08-26T14:28:37.027833Z","shell.execute_reply.started":"2023-08-26T14:26:57.861461Z","shell.execute_reply":"2023-08-26T14:28:37.026697Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"step  500: train loss 2.3963, val loss 2.3826\nstep 1000: train loss 2.2648, val loss 2.2718\nstep 1500: train loss 2.2065, val loss 2.2187\nstep 2000: train loss 2.1526, val loss 2.1648\nstep 2500: train loss 2.1105, val loss 2.1382\nstep 3000: train loss 2.0809, val loss 2.1156\nstep 3500: train loss 2.0466, val loss 2.1006\nstep 4000: train loss 2.0384, val loss 2.1010\nstep 4500: train loss 2.0149, val loss 2.0656\nstep 5000: train loss 2.0047, val loss 2.0675\n","output_type":"stream"}]}]}